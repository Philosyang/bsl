# Machine Learning Overview

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = "center", cache = TRUE, autodep = TRUE)
```

```{r packages, include = FALSE}
library(magrittr)
library(tibble)
library(knitr)
library(kableExtra)
```

This is a book about machine learning, so let's try to define machine learning in this chapter.

Specifically, we'll discuss:

- What is **machine learning**?
- The difference between **supervised learning** and **unsupervised learning**.
- The difference between **classification** and **regression**.

This chapter is currently **under construction**. While it is being developed, the following links to the STAT 432 course notes.

- [**Notes:** Machine Learning Overview](files/ml-tasks.pdf)

## What is Machine Learning?

Machine learning (ML) is about **learning _functions_ from _data_**.^[This is a purposefully narrow view of machine learning. Obviously there's a lot more to ML, but the author believes this statement will help readers understand that the methods learned in this text are simply tools that must be evaluated as a part of a larger analysis.] That's it. Really. Pretty boring, right?^[An alternative title of this book could be: *ML is boring but useful*.]

To quickly address some buzzwords that come up when discussing machine learning:

- **Deep learning** is a subset of machine learning.
- **Artificial Intelligence** (AI) overlaps machine learning but has much loftier goals. In general, if someone claims to be using AI, they are not. They're probably using function learning! For example, we will learn logistic regression in this course. People in marketing might call that AI! Someone who understands ML will simply call it function learning. Don't buy the hype! We don't need to call simple methods AI to make them effective.^[There are certainly people who do legitamtely work on AI, but the strong statement is made here to try to downplay the hype.]
- Machine learning is not **data science**. Data science sometimes uses machine learning.
- Does **big data** exist? If it does, I would bet a lot of money that you haven't seen it, and probably won't see it that often.
- **Analytics** is just a fancy word for doing data analysis. Machine learning can be used in analyses! When it is, it is often called "Predictive Analytics."

What makes machine learning interesting are the uses of these learned functions. We could develop functions that have applications in a wide variety of fields.

In **medicine**, we could develop a function that helps detect skin cancer.

- Input: Pixels from an image of mole
- Output: A probability that the mole is cancerous

In **sport analytics**, we could develop a function that helps determine player salary.

- Input: Lifetime statistics of an NBA player
- Output: An estimate of player's salary

In **meteorology**, we could develop a function to predict the weather.

- Input: Historic weather data in Champaign, IL
- Output: A probability of rain tomorrow in Champaign, IL

In **political science** we could develop a function that predicts the mood of the president.

- Input: The text of a tweet from President Donald Trump
- Output: A prediction of Donald's mood (happy, sad, angry, etc)

In **urban planning** we could develop a function that predicts the rental prices of Airbnbs.

- Input: The attributes of the location for rent
- Output: An estimate of the rent of the property

How do we learn these functions? By looking at many previous examples, that is, data! Again, we will learn functions from data. That's what we're here to do.

## Machine Learning Tasks

When doing machine learning, we will classify our *tasks* into one of two categories, **supervised** or **unsupervised** learning.^[There are technically other tasks such as reinforcement learning and semi-supervised learning, but they are outside the scope of this text. To understand these advanced tasks, you should first learn the basics!] Within these two broad categories of ML tasks, we will define some specifics.

```{r, echo = FALSE}
gen_sup_reg = function(sample_size = 10) {
  x1 = sample(c("A", "B", "C"), size = sample_size, replace = TRUE)
  x2 = round(rnorm(n = sample_size), 2)
  x3 = round(rnorm(n = sample_size), 2)
  y = round(-5 * x2 + 10 + round(rnorm(n = sample_size), 2), 2)
  tibble(x1, x2, x3, y)
}
```

```{r, echo = FALSE}
gen_sup_class = function(sample_size = 10) {
  x1 = sample(c("Q", "P"), size = sample_size, replace = TRUE)
  x2 = round(runif(n = sample_size), 2)
  x3 = round(6 * runif(n = sample_size), 2)
  y = sample(c("A", "B", "C"), size = sample_size, replace = TRUE)
  tibble(x1, x2, x3, y)
}
```

```{r, echo = FALSE}
gen_unsup = function() {
  modify = c(0, 0, 0, 0, 0, 10, 10, 10, 10, 10)
  x1 = round(3 * runif(n = 10), 2) - modify
  x2 = round(runif(n = 10), 2) - modify
  x3 = round(6 * runif(n = 10), 2) - modify
  x4 = round(6 * runif(n = 10), 2) - modify
  x5 = round(6 * runif(n = 10), 2) - modify
  tibble(x1, x2, x3, x4, x5)
}
```

### Supervised Learning

In supervised learning, we want to "predict" a specific *response variable*. (The response variable might also be called the target or outcome variable.) In the following examples, this is the `y` variable. Supervised learning tasks are called **regression** if the response variable is *numeric*. If a supervised learning tasks has a *categorical* response, it is called **classification**.

<!-- TODO: add an appendix chapter about tabular data? -->

#### Regression

In the regression task, we want to predict **numeric** response variables. The non-response variables, which we will call the feature variables, or simply **features** can be either *categorical* or *numeric*.^[These variables are often called "predictor" variables, but we find this nomenclature to be needlessly confusing.]

```{r, echo = FALSE}
set.seed(42)
sup_reg_data = gen_sup_reg()
sup_reg_data %>% 
  kable() %>% 
  kable_styling(full_width = FALSE)
```

With the data above, our goal would be to learn a function that takes as input values of the three features (`x1`, `x2`, and `x3`) and returns a prediction (best guess) for the true (but usually unknown) value of the response `y`. For example, we could obtain some "new" data that does not contain the response.

```{r, echo = FALSE}
gen_sup_reg(sample_size = 1)[, -4] %>% 
  kable() %>% 
  kable_styling(full_width = FALSE)
```

We would then pass this data to our function, which would return a prediction of the value of `y`. Stated mathematically, our prediction will often be an estimate of the conditional mean of $Y$, given values of the $\boldsymbol{X}$ variables. 

$$
\mu(\boldsymbol{x}) = \mathbb{E}[Y \mid \boldsymbol{X} = \boldsymbol{x}]
$$

In other words, we want to learn this function, $\mu(\boldsymbol{x})$. Much more on this later.^[For now, just understand that we are able to make a "best guess" for a new observation.]

#### Classification

Classification is similar to regression, except it considers **categorical** response variables. 

```{r, echo = FALSE}
set.seed(42)
gen_sup_class() %>% 
  kable() %>% 
  kable_styling(full_width = FALSE)
```

As before we want to learn a function from this data using the same inputs, except this time, we want it to output one of `A`, `B`, or `C` for predictions of the `y` variable. Again, consider some new data:

```{r, echo = FALSE}
gen_sup_class(sample_size = 1)[, -4] %>% 
  kable() %>% 
  kable_styling(full_width = FALSE)
```

While ultimately we would like our function to return one of `A`, `B`, or `C`, what we actually would like is an intermediate return of probabilities that `y` is `A`, `B`, or `C`. In other words, we are attempting to estimate the conditional probability that $Y$ is each of the possible categories, given values of the $\boldsymbol{X}$ values.

$$
p_k(\boldsymbol{x}) = P\left[ Y = k \mid \boldsymbol{X} = \boldsymbol{x} \right]
$$

We want to learn this function, $p_k(\boldsymbol{x})$. Much more on this later.

### Unsupervised Learning

Unsupervised learning is a very broad task that is rather difficult to define. Essentially, it is learning without a response variable. To get a better idea about what unsupervised learning is, consider some specific tasks. 

```{r, echo = FALSE}
set.seed(42)
gen_unsup() %>% 
  kable() %>% 
  kable_styling(full_width = FALSE)
```

#### Clustering

Clustering is essentially the task of **grouping** the observations of a dataset. In the above data, can you see an obvious grouping? (Hint: Compare the first five observations to the second five observations.) In general, we try to group observations that are similar.

#### Density Estimation

Density estimation tries to do exactly what the name implies, estimate the density. In this case, the joint density of $X_1, X_2, X_3, X_4, X_5$. In other words, we would like to learn the **function** that generated this data.^[You could take the position that this is the **only** machine learning task, and all other tasks are subset of this task. We'll hold off on explaining this for a while.]

#### Outlier Detection

Consider some new data:

```{r, echo = FALSE}
(gen_unsup()[1, ] + 66) %>% 
  kable() %>% 
  kable_styling(full_width = FALSE)
```

Was this data generated by the same process as the data above? If we don't believe so, we would call it an outlier.

## Open Questions

The two previous sections were probably more confusing than helpful. But of course, because we haven't started learning yet! Hopefully, you are currently pondering one very specific question:

- *How* do we **learn** functions from data?

That's what this text will be about! We will spend a lot of time on this question. It is what us statisticians call fitting a model. On some level the answer is: look at a bunch of old data before predicting on new data. 

While we will dedicate a large amount of time to answering this question, sometimes, some of the details might go unanswered. Since this is an introductory text, we can only go so far. However, as long as we answer another question, this will be OK.

- *How* do we **evaluate** how well learned functions work?

This text places a high priority on being able to **do** machine learning, specifically do machine learning in R. You can actually do a lot of machine learning without fully understanding how the learning is taking place. That makes the *evaluation* of ML models extremely important.
